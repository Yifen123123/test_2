# finetune_e5_mnrl.py
import os
os.environ["TRANSFORMERS_NO_TF"] = "1"   # é¿å… transformers èµ° TF åˆ†æ”¯
os.environ["USE_TF"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["WANDB_MODE"] = "disabled"    # é—œæ‰ wandb äº’å‹•

import json
from pathlib import Path
import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, losses, InputExample

# å»ºè­°æ”¹æˆç´”è‹±æ–‡ã€Œçµ•å°ã€è·¯å¾‘
ROOT          = r"C:\mlproj"
FINETUNE_FILE = str(Path(ROOT) / "finetune_pairs.jsonl")
OUTPUT_DIR    = str(Path(ROOT) / "e5_finetuned_mnrl")
BASE_MODEL    = "intfloat/multilingual-e5-large"

BATCH_SIZE = 16
EPOCHS     = 3
WARMUP_PCT = 0.1

def read_pairs(path):
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                yield json.loads(line)

def build_positive_examples(file):
    """åªå»ºç«‹æ­£ä¾‹å°ï¼štexts=[anchor, positive]ï¼›hard_negative ä¸è¦æ”¾é€²ä¾†ï¼"""
    examples = []
    pos_pairs = 0
    for rec in read_pairs(file):
        a = rec.get("anchor")
        for p in rec.get("positive", []) or []:
            if a and p:
                examples.append(InputExample(texts=[a, p]))
                pos_pairs += 1
    return examples, pos_pairs

def main():
    # ç›®éŒ„é˜²å‘†
    Path(ROOT).mkdir(parents=True, exist_ok=True)
    if not Path(FINETUNE_FILE).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {FINETUNE_FILE}")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ device = {device}")

    # 1) æº–å‚™è³‡æ–™ï¼ˆåªæ­£ä¾‹ï¼‰
    examples, pos_pairs = build_positive_examples(FINETUNE_FILE)
    if len(examples) < 2:
        raise ValueError("æœ‰æ•ˆçš„æ­£ä¾‹å°ä¸è¶³ï¼ˆ<2ï¼‰ã€‚è«‹æª¢æŸ¥ finetune_pairs.jsonl çš„ positive æ¬„ä½ã€‚")
    print(f"ğŸ“‚ æ­£ä¾‹å°æ•¸é‡ = {pos_pairs}ï¼ˆå¯¦éš› examples = {len(examples)}ï¼‰")

    # 2) DataLoaderï¼ˆCPU å‹å–„ï¼‰
    eff_bs = min(BATCH_SIZE, max(2, len(examples)))
    dataloader = DataLoader(
        examples,
        shuffle=True,
        batch_size=eff_bs,
        pin_memory=False,   # CPUï¼šé—œé–‰
        drop_last=False     # å°è³‡æ–™åˆ¥ä¸Ÿæœ€å¾Œä¸€æ‰¹
    )
    steps_per_epoch = len(dataloader)
    print(f"ğŸ§® steps_per_epoch = {steps_per_epoch} | batch_size = {eff_bs}")
    if steps_per_epoch == 0:
        raise ValueError("steps_per_epoch=0ï¼Œè«‹é™ä½ BATCH_SIZE æˆ–å¢åŠ æ¨£æœ¬ã€‚")

    # 3) æ¨¡å‹ & Loss
    model = SentenceTransformer(BASE_MODEL, device=device)
    train_loss = losses.MultipleNegativesRankingLoss(model)

    total_steps  = EPOCHS * steps_per_epoch
    warmup_steps = max(0, int(WARMUP_PCT * total_steps))
    print(f"ğŸš€ è¨“ç·´é–‹å§‹ï¼šepochs={EPOCHS}, total_steps={total_steps}, warmup_steps={warmup_steps}")

    # 4) è¨“ç·´ï¼ˆä¸è½åœ°ï¼ŒçµæŸå¾Œå†æ‰‹å‹•ä¿å­˜ï¼‰
    model.fit(
        train_objectives=[(dataloader, train_loss)],
        epochs=EPOCHS,
        warmup_steps=warmup_steps,
        output_path=None,        # ä¸å­˜ä¸­é€”æª”ï¼Œé¿å… TF/è·¯å¾‘å¹²æ“¾
        checkpoint_path=None,
        show_progress_bar=True,
        use_amp=False
    )

    # 5) ä¿å­˜æœ€çµ‚æ¨¡å‹ï¼ˆåˆ°ç´”è‹±æ–‡è·¯å¾‘ï¼‰
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    model.save(OUTPUT_DIR)
    print(f"âœ… å¾®èª¿å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜ï¼š{OUTPUT_DIR}")

if __name__ == "__main__":
    main()
