# finetune_e5.py
import json
import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, losses, InputExample
from tqdm import tqdm

FINETUNE_FILE = "finetune_pairs.jsonl"
BASE_MODEL    = "intfloat/multilingual-e5-large"
OUTPUT_MODEL  = "./e5_finetuned"

def read_pairs(path):
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                yield json.loads(line)

def build_examples(file):
    examples = []
    for rec in read_pairs(file):
        anchor = rec["anchor"]
        for pos in rec.get("positive", []):
            examples.append(InputExample(texts=[anchor, pos]))
        for neg in rec.get("hard_negative", []):
            # é€™è£¡ä»ç”¨ InputExampleï¼Œbatch å…§æœƒè‡ªå‹•ç•¶ negative
            examples.append(InputExample(texts=[anchor, neg]))
    return examples

def main():
    # 1) åˆ¤æ–·è¨­å‚™
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")

    # 2) è¼‰å…¥ base model
    model = SentenceTransformer(BASE_MODEL, device=device)

    # 3) æº–å‚™è¨“ç·´è³‡æ–™
    examples = build_examples(FINETUNE_FILE)
    print(f"ğŸ“‚ ç¸½å…±è¼‰å…¥ {len(examples)} ç­†è¨“ç·´å°")

    # 4) DataLoaderï¼Œpin_memory è‡ªå‹•ä¾ç’°å¢ƒè¨­å®š
    dataloader = DataLoader(
        examples,
        shuffle=True,
        batch_size=16,
        pin_memory=torch.cuda.is_available()
    )

    # 5) Loss
    train_loss = losses.MultipleNegativesRankingLoss(model)

    # 6) è¨“ç·´
    model.fit(
        train_objectives=[(dataloader, train_loss)],
        epochs=3,
        warmup_steps=10,
        output_path=OUTPUT_MODEL,
        show_progress_bar=True  # å…§å»ºé€²åº¦æ¢
    )

    print(f"âœ… å¾®èª¿å®Œæˆï¼Œæ¨¡å‹å·²è¼¸å‡ºåˆ° {OUTPUT_MODEL}")

if __name__ == "__main__":
    main()
