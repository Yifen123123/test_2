# finetune_e5.py
import os
# 1) åœ¨åŒ¯å…¥ transformers / sentence_transformers ä¹‹å‰ï¼Œå¼·åˆ¶é—œæ‰ TF åˆ†æ”¯
os.environ["TRANSFORMERS_NO_TF"] = "1"
os.environ["USE_TF"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # éœéŸ³ TF è‹¥ç’°å¢ƒé‚„æ˜¯æœ‰å®ƒ
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import json
import math
from pathlib import Path
from typing import List, Dict

import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, losses, InputExample
from tqdm import tqdm

FINETUNE_FILE = "finetune_pairs.jsonl"     # ç”±ä½ çš„è³‡æ–™å»ºç½®è…³æœ¬ç”¢ç”Ÿ
BASE_MODEL    = "intfloat/multilingual-e5-large"
OUTPUT_MODEL  = "./e5_finetuned_triplet"   # è¼¸å‡ºè¦æ˜¯ã€Œè³‡æ–™å¤¾ã€
BATCH_SIZE    = 16
EPOCHS        = 3
MARGIN        = 0.25  # Triplet marginï¼Œå¯èª¿ 0.2~0.5

def read_pairs(path: str) -> List[Dict]:
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    return items

def build_triplets(file: str) -> List[InputExample]:
    """
    æœŸå¾…æ¯è¡Œï¼š
    {"anchor": "...", "positive": ["..."], "hard_negative": ["...", ...]}
    ç”¢ç”Ÿ (anchor, positive, negative) ä¸‰å…ƒçµ„
    """
    raw = read_pairs(file)
    triplets: List[InputExample] = []
    pos_cnt = neg_cnt = 0

    for rec in raw:
        a = rec.get("anchor")
        pos_list = rec.get("positive") or []
        neg_list = rec.get("hard_negative") or []
        # ä¿è­‰ a/pos/neg çš†å­˜åœ¨æ‰çµ„ triplet
        for p in pos_list:
            pos_cnt += 1
            for n in neg_list:
                neg_cnt += 1
                triplets.append(InputExample(texts=[a, p, n]))

    print(f"ğŸ“¦ è®€åˆ° {len(raw)} è¡Œï¼›ä¸‰å…ƒçµ„æ•¸={len(triplets)}ï¼ˆposå°={pos_cnt}, negå€™é¸={neg_cnt}ï¼‰")
    return triplets

def main():
    # 0) åŸºæœ¬å¥æª¢
    finetune_path = Path(FINETUNE_FILE)
    if not finetune_path.exists():
        raise FileNotFoundError(f"{FINETUNE_FILE} ä¸å­˜åœ¨ï¼Œè«‹å…ˆç”Ÿæˆå¾®èª¿è³‡æ–™ã€‚")

    out_dir = Path(OUTPUT_MODEL)
    # æ˜ç¢ºä¿è­‰æ˜¯è³‡æ–™å¤¾ä¸”å¯å¯«å…¥
    out_dir.mkdir(parents=True, exist_ok=True)
    if out_dir.exists() and out_dir.is_file():
        raise NotADirectoryError(f"OUTPUT_MODEL æŒ‡å‘æª”æ¡ˆè€Œéè³‡æ–™å¤¾ï¼š{out_dir}")

    # 1) åˆ¤æ–·è¨­å‚™ï¼ˆCPU é è¨­ï¼‰
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
    print(f"ğŸ“ å°‡è¼¸å‡ºæ¨¡å‹åˆ°è³‡æ–™å¤¾: {out_dir.resolve()}")

    # 2) è¼‰å…¥ base encoderï¼ˆå¼·åˆ¶ PyTorchï¼‰
    model = SentenceTransformer(BASE_MODEL, device=device)

    # 3) æº–å‚™è¨“ç·´ä¸‰å…ƒçµ„
    triplets = build_triplets(FINETUNE_FILE)
    if len(triplets) < 2:
        raise ValueError("Triplet æ¨£æœ¬å¤ªå°‘ï¼ˆ<2ï¼‰ã€‚è«‹æª¢æŸ¥ finetune_pairs.jsonl æ˜¯å¦åŒæ™‚å« positive èˆ‡ hard_negativeã€‚")

    # 4) DataLoaderï¼ˆCPU å‹å–„è¨­å®šï¼‰
    # drop_last=True é¿å…æœ€å¾Œæ‰¹åªæœ‰ 1 ç­†å°è‡´ loss ç„¡æ³•è¨ˆç®—
    eff_batch = min(BATCH_SIZE, max(2, len(triplets)))  # ä¿è­‰ batch >= 2 ä¸”ä¸è¶…éè³‡æ–™æ•¸
    dataloader = DataLoader(
        triplets,
        shuffle=True,
        batch_size=eff_batch,
        pin_memory=False,     # CPU-only æ˜ç¢ºé—œé–‰
        drop_last=True
    )
    steps_per_epoch = len(dataloader)
    if steps_per_epoch == 0:
        raise ValueError(
            f"dataloader ç”¢ç”Ÿ 0 å€‹æ‰¹æ¬¡ã€‚triplets={len(triplets)}, batch_size={eff_batch}\n"
            f"â†’ è«‹é™ä½ BATCH_SIZE æˆ–å¢åŠ è³‡æ–™é‡ã€‚"
        )
    print(f"ğŸ§® steps_per_epoch={steps_per_epoch} (batch_size={eff_batch})")

    # 5) Triplet Lossï¼ˆä»¥ cosine distanceï¼‰
    train_loss = losses.TripletLoss(
        model=model,
        distance_metric=losses.SiameseDistanceMetric.COSINE_DISTANCE,
        triplet_margin=MARGIN
    )

    # 6) è¨“ç·´ï¼ˆé—œé–‰ AMPï¼›åªç”¨ PyTorchï¼‰
    total_steps = EPOCHS * steps_per_epoch
    warmup_steps = max(0, int(0.1 * total_steps))
    print(f"ğŸš€ é–‹å§‹è¨“ç·´ï¼šepochs={EPOCHS}, total_steps={total_steps}, warmup_steps={warmup_steps}")

    model.fit(
        train_objectives=[(dataloader, train_loss)],
        epochs=EPOCHS,
        warmup_steps=warmup_steps,
        output_path=str(out_dir),
        show_progress_bar=True,
        use_amp=False  # é—œé–‰è‡ªå‹•æ··åˆç²¾åº¦ï¼Œé¿å…è·¨å¾Œç«¯å•é¡Œ
    )

    print(f"âœ… å¾®èª¿å®Œæˆï¼Œæ¨¡å‹å·²è¼¸å‡ºåˆ°: {out_dir.resolve()}")

if __name__ == "__main__":
    main()
